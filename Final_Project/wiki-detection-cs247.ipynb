{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7762279,"sourceType":"datasetVersion","datasetId":4539854}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q mlflow datasets>=2.14.5 nlp 2>/dev/null","metadata":{"execution":{"iopub.status.busy":"2024-03-07T16:28:55.422582Z","iopub.execute_input":"2024-03-07T16:28:55.422932Z","iopub.status.idle":"2024-03-07T16:29:27.773227Z","shell.execute_reply.started":"2024-03-07T16:28:55.422894Z","shell.execute_reply":"2024-03-07T16:29:27.772247Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-03-07T16:29:31.093146Z","iopub.execute_input":"2024-03-07T16:29:31.094402Z","iopub.status.idle":"2024-03-07T16:29:44.771542Z","shell.execute_reply.started":"2024-03-07T16:29:31.094354Z","shell.execute_reply":"2024-03-07T16:29:44.770598Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"##### Import Dependencies #####\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport evaluate\nfrom datasets import Dataset, ClassLabel, load_metric\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom transformers import (\n    AutoModel,\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T16:31:24.268525Z","iopub.execute_input":"2024-03-07T16:31:24.269170Z","iopub.status.idle":"2024-03-07T16:31:42.387437Z","shell.execute_reply.started":"2024-03-07T16:31:24.269136Z","shell.execute_reply":"2024-03-07T16:31:42.386636Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-03-07 16:31:31.611936: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-07 16:31:31.612028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-07 16:31:31.734676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"##### Read Data #####\n\n# Read csv\ndf = pd.read_csv('/kaggle/input/wiki-detection/wiki-detection.csv')\n\n# Print stats\nprint(f\"df.shape: {df.shape}\")\nprint(f\"df.columns: {df.columns}\")\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-03-07T16:31:46.894357Z","iopub.execute_input":"2024-03-07T16:31:46.895565Z","iopub.status.idle":"2024-03-07T16:31:50.876748Z","shell.execute_reply.started":"2024-03-07T16:31:46.895531Z","shell.execute_reply":"2024-03-07T16:31:50.875807Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"df.shape: (150000, 2)\ndf.columns: Index(['text', 'label'], dtype='object')\n                                                text  label\n0  Josefine Jakobsen (born 17 May 1991) is a Dani...      0\n1  A cash crop or profit crop is an agricultural ...      0\n2  The Lo Presti 'ndrina of Bardonecchia, known a...      0\n3  John Tran (Vietnamese: Trần Diệc Tuyền; born N...      0\n4  Euthyphro (; ; c. 399–395 BC), by Plato, is a ...      0\n","output_type":"stream"}]},{"cell_type":"code","source":"##### Create Dataset #####\n\n# Class type\nclasses = [0, 1]\n\n# Testing set fraction\ntest_fraction = 0.9\n\n# Dataset\nunsplitted_dataset = Dataset.from_pandas(df)\n\n# Construct class label\nClassLabels = ClassLabel(num_classes=2, names=classes)\nunsplitted_dataset = unsplitted_dataset.cast_column('label', ClassLabels)\n\n# Split the dataset\ndataset = unsplitted_dataset.train_test_split(test_size=test_fraction, shuffle=True, stratify_by_column='label')","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:52:53.154725Z","iopub.execute_input":"2024-03-07T21:52:53.155123Z","iopub.status.idle":"2024-03-07T21:52:54.682114Z","shell.execute_reply.started":"2024-03-07T21:52:53.155094Z","shell.execute_reply":"2024-03-07T21:52:54.681326Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/150000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ade933a37ad47e991292fdbc3966303"}},"metadata":{}}]},{"cell_type":"code","source":"##### Language Model related Setup #####\n\n# Language model type\nlm_type = 'bert-base-uncased'\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(lm_type)\n\n# Model\nlm_encoder = AutoModel.from_pretrained(lm_type)\nlm_classifier = AutoModelForSequenceClassification.from_pretrained(lm_type, num_labels=2)\n\n# Encode text data\ndef encode_data(example):\n    encoding = tokenizer(example['text'], truncation=True)\n    encoding['label'] = example['label']\n    return encoding\n\ndataset = dataset.map(encode_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:52:57.180209Z","iopub.execute_input":"2024-03-07T21:52:57.180828Z","iopub.status.idle":"2024-03-07T21:55:44.752481Z","shell.execute_reply.started":"2024-03-07T21:52:57.180796Z","shell.execute_reply":"2024-03-07T21:55:44.751565Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008713f10bb94c9eb3619ef6ef68b538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/135000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a5fa7438d6496b9628123226b9037f"}},"metadata":{}}]},{"cell_type":"code","source":"##### Defining Metric of Accuracy #####\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-07T16:35:11.095931Z","iopub.execute_input":"2024-03-07T16:35:11.096278Z","iopub.status.idle":"2024-03-07T16:35:11.842245Z","shell.execute_reply.started":"2024-03-07T16:35:11.096248Z","shell.execute_reply":"2024-03-07T16:35:11.841219Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d2527a57cc947c2a359efe09a476fe1"}},"metadata":{}}]},{"cell_type":"code","source":"##### Setup device #####\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlm_classifier = lm_classifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:55:52.850659Z","iopub.execute_input":"2024-03-07T21:55:52.851487Z","iopub.status.idle":"2024-03-07T21:55:52.971089Z","shell.execute_reply.started":"2024-03-07T21:55:52.851453Z","shell.execute_reply":"2024-03-07T21:55:52.970273Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"##### Language Model Training Setup #####\n\n# Training hyperparameters\nnum_train_epochs = 1\nlearning_rate = 2e-7\ntrain_batch_size = 8\neval_batch_size = 64\nwarmup_steps = 50\nweight_decay = 0.02\noutput_dir = f\"wiki-generated-intro-detection-{lm_type}\"\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Defining training arguments\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=train_batch_size,\n    per_device_eval_batch_size=eval_batch_size,\n    evaluation_strategy=\"epoch\",\n    warmup_steps=warmup_steps,\n    weight_decay=weight_decay,\n    logging_dir=\"./logs\",\n    num_train_epochs=num_train_epochs,\n    save_steps=1000,\n    save_total_limit=1,\n    report_to=\"mlflow\"\n)\n\n# Set trainer\ntrainer = Trainer(\n    model=lm_classifier,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:55:55.991967Z","iopub.execute_input":"2024-03-07T21:55:55.992315Z","iopub.status.idle":"2024-03-07T21:55:56.027313Z","shell.execute_reply.started":"2024-03-07T21:55:55.992288Z","shell.execute_reply":"2024-03-07T21:55:56.026326Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"##### Training and Evaluation #####\n\n# Train\ntrainer.train()\n\n# Evaluate\n# results = trainer.evaluate()\n# print(results)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T21:55:59.370570Z","iopub.execute_input":"2024-03-07T21:55:59.370944Z","iopub.status.idle":"2024-03-07T22:40:19.645113Z","shell.execute_reply.started":"2024-03-07T21:55:59.370915Z","shell.execute_reply":"2024-03-07T22:40:19.644024Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 44:19, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.056700</td>\n      <td>0.134925</td>\n      <td>0.976215</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1875, training_loss=0.10609483439127604, metrics={'train_runtime': 2659.4346, 'train_samples_per_second': 5.64, 'train_steps_per_second': 0.705, 'total_flos': 2572153121648160.0, 'train_loss': 0.10609483439127604, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Geberating LM Embeddings","metadata":{}},{"cell_type":"code","source":"##### Get Different Models #####\n\nfrom functools import partial\nfrom tqdm.notebook import tqdm\n\n# Language model type\n\nlm_type_list = [\n#     'distilbert-base-uncased',\n    'bert-base-uncased',\n#     'microsoft/deberta-base',\n]\n    \n# Encode text data\ndef encode_dataset(example, lm_tokenizer):\n    encoding = lm_tokenizer(example['text'], truncation=True)\n    encoding['label'] = example['label']\n    return encoding\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:55:06.654314Z","iopub.execute_input":"2024-03-05T15:55:06.655116Z","iopub.status.idle":"2024-03-05T15:55:06.660353Z","shell.execute_reply.started":"2024-03-05T15:55:06.655084Z","shell.execute_reply":"2024-03-05T15:55:06.659470Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"##### Setup device #####\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-05T14:53:02.407966Z","iopub.execute_input":"2024-03-05T14:53:02.408352Z","iopub.status.idle":"2024-03-05T14:53:02.435321Z","shell.execute_reply.started":"2024-03-05T14:53:02.408321Z","shell.execute_reply":"2024-03-05T14:53:02.433977Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"##### Generating and Saving the Embedding #####\n\nfrom torch.utils.data import DataLoader\n\nnum_samples = len(unsplitted_dataset)\n\nfor lm_name in lm_type_list:\n    saving_name = lm_name.split('/')[-1]\n    print(f\"Dealing with {saving_name} ...\")\n    cur_tokenizer = AutoTokenizer.from_pretrained(lm_name)\n    cur_model = AutoModel.from_pretrained(lm_name).to(device)\n\n    encode_dataset_partial = partial(encode_dataset, lm_tokenizer=cur_tokenizer)\n    cur_dataset = unsplitted_dataset.map(encode_dataset_partial)\n\n    # Initialize a empty numpy array to save the embedding\n    cls_embeddings = np.zeros((num_samples, cur_model.config.hidden_size))\n\n    # Iterate the dataset\n    for i, example in tqdm(enumerate(cur_dataset)):\n        item = {}\n        item['attention_mask'] = torch.IntTensor(np.array(example['attention_mask'])).unsqueeze(0).to(device)\n        item['input_ids'] = torch.IntTensor(np.array(example['input_ids']).astype(np.int32)).unsqueeze(0).to(device)\n        if saving_name not in ['distilbert-base-uncased']:\n            item['token_type_ids'] = torch.IntTensor(np.array(example['token_type_ids'])).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            lm_outputs = cur_model(**item)\n            emb = lm_outputs.last_hidden_state\n            cls_embeddings[i] = emb.permute(1, 0, 2)[0].cpu().numpy()\n    np.save(f'wiki-output/{saving_name}_embeddings.npy', cls_embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:55:12.947990Z","iopub.execute_input":"2024-03-05T15:55:12.948348Z","iopub.status.idle":"2024-03-05T16:55:32.515996Z","shell.execute_reply.started":"2024-03-05T15:55:12.948319Z","shell.execute_reply":"2024-03-05T16:55:32.515145Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Dealing with deberta-base ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcc07e02861426cbd9cf21c3ccbda94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b89c7e2d42264095a8886cc18476cc7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12188f15c5d244079828f8bb1307b141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1312b9cf714647b6947fe0a2f82d47fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"761a0c7553164dfbbbcd94b27696db13"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b53e3a340b414defbd548c75b9a97aee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2fc815fed83423080d0fc5431df77b8"}},"metadata":{}}]},{"cell_type":"code","source":"##### Save Label #####\n\nnp.save('wiki-output/wiki-label.npy', np.array(unsplitted_dataset['label']))","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:59:09.859404Z","iopub.execute_input":"2024-03-05T16:59:09.859779Z","iopub.status.idle":"2024-03-05T16:59:09.971335Z","shell.execute_reply.started":"2024-03-05T16:59:09.859749Z","shell.execute_reply":"2024-03-05T16:59:09.970300Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 4. MLP with fixed LM embedding","metadata":{}},{"cell_type":"code","source":"##### Read the Data #####\n\n\"\"\"\nLM emb type:\n'roberta-base',\n'albert-base-v2',\n'distilbert-base-uncased',\n'bert-base-uncased',\n'deberta-base',\n\"\"\"\nlm_type_list = [\n    'distilbert-base-uncased',\n    'bert-base-uncased',\n    'deberta-base',\n]\nroot_path = 'wiki-output/'\nemb_path_list = [root_path + emb_type + '_embeddings.npy' for emb_type in lm_type_list]\nlabel_path = root_path + 'wiki-label.npy'\n\n# Read\nemb_list = [np.load(emb_path) for emb_path in emb_path_list]\nlabel = np.load(label_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:03:38.298454Z","iopub.execute_input":"2024-03-05T17:03:38.298809Z","iopub.status.idle":"2024-03-05T17:03:39.189794Z","shell.execute_reply.started":"2024-03-05T17:03:38.298782Z","shell.execute_reply":"2024-03-05T17:03:39.189023Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"##### Construct Dataset #####\n\n# Import dependencies\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# Construct a dataset\nclass EmbDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Dataset and split\nN, D = emb_list[0].shape\nemb_dataset_list = [EmbDataset(emb, label) for emb in emb_list]\nemb_train_fraction = 0.8\nemb_train_size = int(emb_train_fraction * N)\nemb_valid_size = N - emb_train_size\nemb_train_valid = [random_split(emb_dataset, [emb_train_size, emb_valid_size]) for emb_dataset in emb_dataset_list]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:06:53.537654Z","iopub.execute_input":"2024-03-05T17:06:53.538456Z","iopub.status.idle":"2024-03-05T17:06:53.580279Z","shell.execute_reply.started":"2024-03-05T17:06:53.538423Z","shell.execute_reply":"2024-03-05T17:06:53.579258Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"##### MLP model #####\n\n# MLP model\nclass MLP(nn.Module):\n    def __init__(self, input_size, output_size, hidden_layers):\n        super(MLP, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n\n        assert len(hidden_layers) > 0\n        self.hidden_layers = nn.ModuleList([])\n        for i, layer_size in enumerate(hidden_layers):\n            if i == 0:\n                self.hidden_layers.append(nn.Linear(input_size, layer_size))\n            else:\n                self.hidden_layers.append(nn.Linear(hidden_layers[i-1], layer_size))\n\n        self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        for layer in self.hidden_layers:\n            x = self.relu(layer(x))\n        x = self.output_layer(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:07:02.279089Z","iopub.execute_input":"2024-03-05T17:07:02.279973Z","iopub.status.idle":"2024-03-05T17:07:02.288134Z","shell.execute_reply.started":"2024-03-05T17:07:02.279937Z","shell.execute_reply":"2024-03-05T17:07:02.287235Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"##### Initialize Model #####\n\n# Model hyperparameters\ninput_size = D\noutput_size = 2\nhidden_layers = [256, 128]\n\n# Training hyperparameters\nnum_epochs = 10\nbatch_size = 64\nlearning_rate = 1e-3\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:09:21.654774Z","iopub.execute_input":"2024-03-05T17:09:21.655490Z","iopub.status.idle":"2024-03-05T17:09:21.659999Z","shell.execute_reply.started":"2024-03-05T17:09:21.655453Z","shell.execute_reply":"2024-03-05T17:09:21.659082Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"##### Train and Eval #####\n\nfrom sklearn.metrics import roc_auc_score\n\n# Define eval\ndef eval(mlp_model, emb_valid_loader):\n    mlp_model.eval()\n    correct = 0\n    total = 0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for batch_emb, batch_label in emb_valid_loader:\n            batch_emb = batch_emb.to(device)\n            batch_label = batch_label.to(device)\n            outputs = mlp_model(batch_emb.float())\n            _, predicted = torch.max(outputs.data, 1)\n            total += batch_label.size(0)\n            correct += (predicted == batch_label).sum().item()\n            y_true.extend(batch_label.cpu().numpy())\n            y_pred.extend(outputs[:, 1].cpu().numpy())\n    print(f\"Validation Accuracy: {100 * correct / total}%\")\n    print(f\"Validation AUC: {roc_auc_score(y_true, y_pred)}\")\n\n# Define training\ndef train(mlp_model, emb_train_loader, emb_valid_loader, criterion, optimizer):\n    for epoch in range(num_epochs):\n        mlp_model.train()\n        total_loss = 0\n        for batch_emb, batch_label in emb_train_loader:\n            batch_emb = batch_emb.to(device)\n            batch_label = batch_label.to(device)\n            optimizer.zero_grad()\n            outputs = mlp_model(batch_emb.float())\n            loss = criterion(outputs, batch_label)\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch {epoch+1}, Training Loss: {total_loss / len(emb_train_loader)}\")\n        eval(mlp_model, emb_valid_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:17:25.325161Z","iopub.execute_input":"2024-03-05T17:17:25.326239Z","iopub.status.idle":"2024-03-05T17:17:25.336166Z","shell.execute_reply.started":"2024-03-05T17:17:25.326202Z","shell.execute_reply":"2024-03-05T17:17:25.335347Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"##### Training #####\n\nfor i in range(3):\n    print(f'Now dealing with {lm_type_list[i]}')\n    # Model initialization\n    mlp_model = MLP(input_size, output_size, hidden_layers).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n\n    # Dataloader\n    emb_train_loader = DataLoader(emb_train_valid[i][0], batch_size=batch_size, shuffle=True)\n    emb_valid_loader = DataLoader(emb_train_valid[i][1], batch_size=batch_size, shuffle=False)\n    \n    # Train\n    train(mlp_model, emb_train_loader, emb_valid_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:17:29.093706Z","iopub.execute_input":"2024-03-05T17:17:29.094285Z","iopub.status.idle":"2024-03-05T17:19:45.009959Z","shell.execute_reply.started":"2024-03-05T17:17:29.094252Z","shell.execute_reply":"2024-03-05T17:19:45.009079Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Now dealing with distilbert-base-uncased\nEpoch 1, Training Loss: 0.1609278850749135\nValidation Accuracy: 95.58333333333333%\nValidation AUC: 0.9926686803986947\nEpoch 2, Training Loss: 0.10632828851789236\nValidation Accuracy: 96.61666666666666%\nValidation AUC: 0.995314733190388\nEpoch 3, Training Loss: 0.09041621039013067\nValidation Accuracy: 96.54%\nValidation AUC: 0.9951538233133905\nEpoch 4, Training Loss: 0.08168215119813879\nValidation Accuracy: 95.77%\nValidation AUC: 0.9963725542950944\nEpoch 5, Training Loss: 0.07451102772740026\nValidation Accuracy: 97.08666666666667%\nValidation AUC: 0.9964761874738025\nEpoch 6, Training Loss: 0.06651370524714391\nValidation Accuracy: 97.09333333333333%\nValidation AUC: 0.9967458506316598\nEpoch 7, Training Loss: 0.06149057482307156\nValidation Accuracy: 96.99%\nValidation AUC: 0.9968226877202009\nEpoch 8, Training Loss: 0.058821020236611364\nValidation Accuracy: 97.16333333333333%\nValidation AUC: 0.997116640251547\nEpoch 9, Training Loss: 0.054662465528585016\nValidation Accuracy: 97.41666666666667%\nValidation AUC: 0.9974183973830484\nEpoch 10, Training Loss: 0.05179659345659117\nValidation Accuracy: 97.46333333333334%\nValidation AUC: 0.9974356688387442\nNow dealing with bert-base-uncased\nEpoch 1, Training Loss: 0.1258164347494642\nValidation Accuracy: 96.76666666666667%\nValidation AUC: 0.9958729569001322\nEpoch 2, Training Loss: 0.07753911285201709\nValidation Accuracy: 96.90666666666667%\nValidation AUC: 0.9965211018314053\nEpoch 3, Training Loss: 0.06407635998874903\nValidation Accuracy: 97.45%\nValidation AUC: 0.9971664200938889\nEpoch 4, Training Loss: 0.05530734508695702\nValidation Accuracy: 96.49%\nValidation AUC: 0.9973700113579196\nEpoch 5, Training Loss: 0.04988246170009176\nValidation Accuracy: 97.57666666666667%\nValidation AUC: 0.9974194602839501\nEpoch 6, Training Loss: 0.04367898802946632\nValidation Accuracy: 97.79666666666667%\nValidation AUC: 0.9979680829182489\nEpoch 7, Training Loss: 0.038580478060462824\nValidation Accuracy: 97.70333333333333%\nValidation AUC: 0.9977249760689821\nEpoch 8, Training Loss: 0.03457164684737412\nValidation Accuracy: 97.83666666666667%\nValidation AUC: 0.9979727406995252\nEpoch 9, Training Loss: 0.031890396342535196\nValidation Accuracy: 97.76666666666667%\nValidation AUC: 0.9978794872961483\nEpoch 10, Training Loss: 0.028438824361690786\nValidation Accuracy: 97.89333333333333%\nValidation AUC: 0.9980442429754536\nNow dealing with deberta-base\nEpoch 1, Training Loss: 0.26062134797970454\nValidation Accuracy: 93.09333333333333%\nValidation AUC: 0.9914231710729919\nEpoch 2, Training Loss: 0.12815801296432813\nValidation Accuracy: 96.75%\nValidation AUC: 0.9956048599804662\nEpoch 3, Training Loss: 0.1019557712689042\nValidation Accuracy: 97.34333333333333%\nValidation AUC: 0.9967102177631565\nEpoch 4, Training Loss: 0.09350253313581149\nValidation Accuracy: 90.25333333333333%\nValidation AUC: 0.9973073755435884\nEpoch 5, Training Loss: 0.08760747638344765\nValidation Accuracy: 95.86333333333333%\nValidation AUC: 0.9974815377665847\nEpoch 6, Training Loss: 0.08248523042947054\nValidation Accuracy: 97.74333333333334%\nValidation AUC: 0.9979478888797684\nEpoch 7, Training Loss: 0.0758003221017619\nValidation Accuracy: 97.47%\nValidation AUC: 0.9981220222138757\nEpoch 8, Training Loss: 0.07795612498720488\nValidation Accuracy: 98.03666666666666%\nValidation AUC: 0.9980991244359961\nEpoch 9, Training Loss: 0.0747048267359535\nValidation Accuracy: 96.34666666666666%\nValidation AUC: 0.9983180133258578\nEpoch 10, Training Loss: 0.07354143631371358\nValidation Accuracy: 96.45333333333333%\nValidation AUC: 0.9983015155480067\n","output_type":"stream"}]}]}